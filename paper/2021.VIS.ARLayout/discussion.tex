\section{Discussion and Future Work}
\label{sec:discussion}

We discuss the limitations and some potential extension work of this paper
in this section.

%\subsection{Limitations}

We design a personal visualization tool named \textit{ARLayout}
for non-experts without programming knowledge
to build AR-based visual re-layouts towards massive physical targets.
We summarize the salabilities and some limitations of \textit{ARLayout} as follows:

\textbf{The scalability of application scenarios}. Except the usage scenarios presented in the paper,
the current version of \textit{ARLayout} supports various application scenarios where:
(1) the targets with textual information such as menus of coffee, drinks, foods, etc.,
goods in supermarkets with name-price labels,
or even a large number of cars in a parking lot (license plates), etc.
(2) the targets with colors such as eyeshadow,
colored balls or balloons in a large amusement park,
the colored goods in supermarkets, etc.
(3) the targets with 2-D shapes and some simple 3-D shapes like tables, chairs, windows, etc., 
however, it is not applicable for complex 3-D shapes due to
the limitations of image labelling algorithms towards various 3-D objects. 

\textbf{Rendering limitation of AR devices}.
We used the latest 11-inch 2021 iPad-Pro with A12Z processor,
which is one of the top end configuration for the nowadays iPad with ARKit.
The framework could visualize the AR targets (actually are virtual targets)
in AR environment well when the number is below 200.
However, as the target number increases, the frame rate drops.
%This situation appears more in the library/bookstore scenario, where the number of books often exceeds 1000.
Though several experiments, we found that the frame rate stabilizes at 60 per second
when the number of targets (e.g., books in the library scenario) is below 200, and decreases to 30 frames per second
if it exceeds 230. Furthermore, if AR books' number increases to 320, the frame rate
will drop significantly to below 15.

Nevertheless, this limitation (low FPS when the target number grows larger than 320)
mainly restricted by the current mobile device would \textbf{not appear} if users
follow a rountine task workflow, i.e.,
searching the targets first to narrow down the number of candidates to 320 or less.
The more unrelated targets are filtered out, the more efficient and the more effective
the users can fulfill their tasks.
For example, users can search books with a fuzzy keyword ``data structure''
to decrease the data space significantly,
and then the subsequent tasks including re-grouping, re-ranking or further searching
will be visualized fluently enough in AR environment.
The major issue of this limitation is that the AR device can not show
too many AR objects (e.g., larger than 320).

It is worth noting that the image segmentation and labelling components of \textit{ARLayout}
are \textbf{scalable} and \textbf{not limited} by the target number,
because the convolutional neural network and the OCR algorithm are run on the server
which can even handle thousands of books in the library usage scenario in our experiments.
More importantly, unlike the mobile device, the computation resources of the server are
\textbf{scalable enough} and which could be upgraded easily.
As a result, while \textit{ARLayout} recognizes almost all the books scanned by the user,
we recommend the user to first filter out unrelated books by fuzzy searching before actually
visualizing those books in the AR space in order to narrow down the data space.


\textbf{Image resolution limitation}. \textit{ARLayout} recognizes objects by images taken from the mobile devices.
Ideally, the user only need to take one panoramic picture that contains all the target objects.
However, targets' details may not be recognized if they are too small in the picture,
that is the user is stand too far away from the massive targets.
For example, in the library/bookstore scenario, instead of scanning all layers of the bookshelves,
the user may walk closer to the bookshelves and scan one layer at one time by panoramic stitching
due to lack of light or limited imaging quality.



%\subsection{Future Work}
We plan to optimize the following aspects of \textit{ARLayout} in future work:

\textbf{More complex application scenarios support.} Currently \textit{ARLayout} is capable of recognizing books, coffee names,
drinking menu, food menu, or eyeshadows.
In the future, we plan to expand its usage scenarios to other similar ones like choosing cups, fruits or flowers, etc.
Because targets with text on it, or in different colors and shapes can be well recognized by trained neural networks.
During targets recognition, targets' additional information is acquired from the database,
so an interface for fast data import can be implemented
in future work to meet the data requirements of more scenarios.

%\textbf{Real-time position sync between virtuality and reality.}
%The current version of \textit{ARLayout} can
%accurately creates virtual targets in the AR environment
%whose position is consistent with the real world targets.
%% and is able to accept simple changes in the scenario,
%such as following the targets shaking,
%but there is still a long way to go with the dynamic scenarios.
%For example, it is extremely difficult for the current \textit{ARLayout} to use AR
%to dynamically capture the changes of players in a football match.
%But the basic algorithm of AR,
%such as plane tracking and 3-D targets recognition,
%have reached quite performance requirements.
%Therefore, the difficulty of moving \textit{ARLayout} from static scenario to dynamic scenario
%depends on the real-time data processing.

\textbf{Detailed Texture Recognition in AR}. Some of user study participants
consider it could be better to use the same texture to show the AR effects,
especially in the eyeshadow scenario.
At present, the preview effect of eyeshadow only stays at the transfer of RGB value.
Further study with more focus on detailed texture recognition is therefore suggested,
e.g., the eyeshadow texture on glossiness or oiliness, etc.
A Deep Texture Encoding Network (DeepTEN) with a novel Encoding Layer
integrated on top of convolutional layers offers possibilities
to solve this problem~\cite{Zhang2017}.
Besides, we are trying to use a better architecture,
such as abandoning the server if the mobile device is affordable
and building a lightweight neural network to move them to run on the mobile devices.



% \textbf{Interface for data import}
% During targets recognition, targets' additional information is aquired from the database.
% which means that in similar usage scenario,
% \textit{ARLayout} can also provide data support through pattern matching algorithm.
% In other words,
% an interface for fast data import can be implemented
% in future work to meet the data requirements of more scenarios.

%\textbf{Automatic scenario recognition}
%When using the current ARLayout,
%users need to classify the current scenario type,
%and then choose the corresponding recognition mode,
%which often leads to inconvenience in using.
%The complexity of the scenario requires
%\textit{ARLayout} to be designed with scenario understanding
%to avoid the difficulty of describing the current scenario type
%when users facing an unknown scenario.
%Recently, the research on computer vision on visualization
%has provided AI's scenario automatic recognition function,
%which make it possible for environment adaptive visualization~\cite{Andreu2011}.

% \textbf{Relatedwork Comparison}
% The most related work of \textit{ARLayout} are some authoring tools towards AR or VR visualizations.
% As shown in Table~\ref{tab:discussion_relatedwork_cmp},
% we compare them from eight aspects:
% highlight(fisheye), collabration, searching, re-grouping, re-ranking,
% massive targets, glyph vis, augmented infomation.
% We summarize them into six criterias:
% more intuitive interaction, personal or collaborated,
% layout changes or keep origin layout,
% a few targets or massive targets, whether glyph used,
% use the original information or add more information.

% \begin{table}[htp]
%     \centering
%     \includegraphics[width=\linewidth]{images/discussion_relatedwork_cmp.eps}
%     \caption{
%         Comparison to the most related recent work about authoring tools
%         towards AR or VR visualizations.
%     }
%     \label{tab:discussion_relatedwork_cmp}
% \end{table}
