\section{Related Work}
\subsection{AR Visualizations}

Embedded data representations links visualization systems to physical things~\cite{Willett2017}.
Augmented Reality (AR), as an important method to combine digital data and physical world,
can visualize data in the physical space to facilitate certain visual explorations and integrate visualization into personal ideas and preferences~\cite{Butscher2018}.
When displaying ubiquitous data integrated into the everyday life,
spatial immersion issues like depth perception, data localization and object relations become relevant.
Works related to AR nowadays can be roughly classified as mobile (or tablets) hand-based systems~\cite{ElSayed2015},
and head-mounted displays (HMD) systems~\cite{Herr2017} according to the computing paradigm.
The Hololens device includes a depth sensing camera that estimates the
distance of each pixel in view and stitches together a mesh
or spatial map
of the environment~\cite{Evans2017}.
Similar technologies are also available in Google Tango~\cite{MarderEppstein2016} and Intel Realsense~\cite{Keselman2017}.
The software development kit (SDK)~\cite{Amin2015} provides ordinary programmers
with more freedom and flexibility
to use their own inspiration to design excellent AR applications,
such as ARToolkit~\cite{Wagner2003, Hirokazu2002},
Vuforia~\cite{Cushnan2013},
ARCore for Android~\cite{ArCore},
and Mixed for Microsoft Universal Windows platform (UWP) Reality Toolkit~\cite{Liberty2018}.
A-Frame~\cite{aframe} which enables create immerse visualization scenes in the browser integrating by WebVR~\cite{webvr} content within HTML.

An inevitable direction of AR development is simultaneous localization and mapping
(SLAM)~\cite{Davison2003} technology, which can build a model simulating the real environment through the background process based on panoramic 3-D reconstruction.
Motion tracking based on MonoSLAM~\cite{Davison2007} has problems such as extensive calculation,
long work time, existing scale ambiguity~\cite{Davison2007}, and difficulty detecting feature points when the device is moving fast.
The integration of inertial measurement unit IMU~\cite{Jones2011} to get six degrees of freedom (6DOF)~\cite{Leutenegger2015} of the device
plays a complementary role in improving its refresh rate and accuracy.

AR visualization has been applied to many fields.
In outdoor visualization,
the CityViewAR~\cite{Lee2012} provides information about destroyed buildings and historical sites that are affected by the earthquakes.
Then,
AR in the interpretation of terrain relief~\cite{Carrera2018} shows a great usability,
which serves as a motivational tool for the 3-D visualization.
Applications in the newly emerging field of Augmented Reality Art
show the paradigmatic potential of AR as a new artistic medium~\cite{Geroimenko2012} .

\subsection{Personal Visualization}
Personal Visualization fits personal routines, situating visualization in a real-world context, and arousing users' interests.
Visualization (Vis) and visual analytics (VA) offer substantial opportunities to help individuals gain insights about themselves.
The combination of large interactive displays with personal head-mounted Augmented Reality (AR)~\cite{Reipschlager2021} for information visualization to facilitate data exploration and analysis.
Personal Visualization and Personal Visual Analytics (PV \& PVA) are defined in terms of personal context~\cite{Huang2014}.
It broadens the scope of visualization and visual analytics,
including casual InfoVis~\cite{Pousman2007}, InfoVis for the Masses~\cite{Danziger2008},
persuasive computing~\cite{Fogg1998} and personal informatics (PI)~\cite{Li2010, Li2010a}.
Researchers have presented mobile PV systems~\cite{Chiu2009, Kim2010, Kjeldskov2012}
to visualize daily information collected from peripheral sensors.
Besides, in physical PV systems express abstract data in a physical form~\cite{Jansen2015}.
Jansen et al.~\cite{Jansen2013, Jansen2015a} shows physical visualization can improve the users' efficiency in retrieval tasks.

Some researchers have designed physical visualization to visualize personal data.
Khot et al.~\cite{Khot2014} transforms heart rate data into five kinds of 3-D printed material artifacts,
and Stusak et al.~\cite{Stusak2014}designs four types of sculptures to visualize running activities and
evaluates their usability through a field study.
Huang et al.~\cite{Huang2016} designs a behaviour feedback visualization tool
to help people improve their health or personal well-being or to carry out sound environmental sustainability practices.
Moreover, understanding and reasoning about personal data is of great importance,
e.g., pedometer counts~\cite{Stusak2014}, blood pressure readings~\cite{Goh2016} or home electricity consumption~\cite{Monigatti2010},
It can be chanllenging while gaining a deeper understanding of one's current practices and learning how to make a change when using data alone.

\subsection{Visualization Authoring Tools}
The field of immersive visualization makes use of Augmented Reality techniques to successfully support users in visualizing data.
However,
designing visualizations in immersive environments can be intricate,
which needs consideration of issues such as shadows~\cite{Allen1999, Mamassian1998, Yonas1978}, occlusion, and additional argumentations.
Nowadays several techniques are available that allow real-time rendering of shadows in augmented reality scenes~\cite{Tsuda2006, Wither2005}.
Different approaches are used to modify the appearance of occluding objects to uncover the hidden ones.
Cut-aways can be found for instance~\cite{Bichlmeier2007, Furmanski2002},
and transparencies in combination with masks are used~\cite{Mendez2009}.

On one hand,
researches have proposed several toolkits that allows interactive authoring and exploration of data visualisation in immersive environments.
With MARVisT~\cite{Chen2020}, users without visualization expertise can bind data to real-world objects to create expressive AR glyph-based visualizations rapidly and effortlessly.
DXR~\cite{Sicat2019} further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world.
IATK~\cite{Cordeil2019} allows for easy assembly of visualisations through a grammar of graphics that a user can configure in a GUI--in addition to a dedicated visualisation API.
PapARVis~\cite{Chen2020a} designs an environment that can debug both static and virtual content simultaneously.
Automated Window/Icon/Menu/Pointing Device User Interface (WIMP-UI)~\cite{WIMPUI} generation has been considered a promising technology for at least two decades.
iVisDesigner~\cite{Ren2014} achieves high interactive expressiveness through conceptual modularity, covering a broad information visualization design space.
A mixed-initiative system Voyager that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures~\cite{Wongsuphasawat2016}.

On the other hand,
an API known as OpenGL bridges the gap between piles of raw data and extremely complicated three-dimensional animation in a way~\cite{Shreiner2013}.
The graphical demands of some of the applications have impeded their successful settlement in Web scenarios,
thus people begin to use WebGL~\cite{Parisi2012} to articulates a large portion of the rendering task in the client machine.
Declarative visual design language can customizes built-in chart types, such as Echarts~\cite{Li2018} and D3~\cite{Zhu2013}.
A high-level grammar Vega-lite~\cite{Satyanarayan2017} provides visual encoding rules and a composition algebra that enables rapid specification of interactive data visualizations.
Some features are used to analyze, display and manage the 3-D structure of the molecules by Vega~\cite{Pedretti2002}.

\begin{table}[htp]
%\setlength{\abovecaptionskip}{0.05cm}
%\setlength{\belowcaptionskip}{-0.4cm}
    \centering
    \includegraphics[width=\linewidth]{images/discussion_relatedwork_cmp.eps}
    \caption{
        Comparison to the most related recent work about authoring PV tools towards AR or VR visualizations.
        The most related approaches include:
        DXR~\cite{Sicat2019}, Augmented Virtual Teleportation (AVT)~\cite{Rhee2020}, 
        Situated Analytics (SA Vis)~\cite{ElSayed2015}, Data Visceralization (VR Visc)~\cite{Lee2021a},
        Shared Surfaces and Spaces (VR Collab Vis)~\cite{Lee2021}, PapARVis~\cite{Chen2020a},
        MARVisT~\cite{Chen2020}.
        The workflow can be categorized into PV (single user in a personal context),
        single user or collaborative users.
    }
    \label{tab:discussion_relatedwork_cmp}
\end{table}



\subsection{Relationship with The Most Related Work}
\label{sec:relatedworkcomparison}

We note that there are some most related AR-based tools on authoring visualizations~\cite{Chen2020,Chen2020a}, etc.
We summarize and discuss the differences between \textit{ARLayout} and the most related ones
as shown in Table~\ref{tab:discussion_relatedwork_cmp}
according to the data scale, tasks (augmented information, searching, re-grouping, re-ranking),
visual presentations (glyph, small multiples, fish eye highlight), workflow (personal, single or collaborative).

First, the biggest differences between our work and the existing AR-based PV tools like MARVisT~\cite{Chen2020}
are the data scale and the tasks, we focus on massive targets especially for
the number is larger than 40 or even thousands of targets like books in a library/bookstore
while most of AR-based PV tools just focus on physical targets with the number
smaller than $30$~\cite{Chen2020}, PapARVis~\cite{Chen2020a} ($\leq 5$) and Situated Analytics~\cite{ElSayed2015} ($\leq 5$).
The large data scale of this paper poses a new challenge in image segmentation, object labelling, textual information
recognition and the AR-based data presentation.

Second, we focus on searching/filtering, re-grouping and re-ranking according to
the additional attributes of massive physical targets
through the visual re-layouts of them,
instead of augmenting the existing static visualizations in PapARVis~\cite{Chen2020a}.
The PV tasks are different from the most related work due to the larger data scale of \textit{ARLayout}.
On the one hand, our work can process more than a thousand candidate targets at the same time,
allowing users to access most of the information in the scenario.
On the other hand, this feature on data scale also makes our work adaptive to more scenarios,
rather than limited to some specific scenarios with only a few targets.

Third, the visual presentations of \textit{ARLayout} are different.
We focus on AR-based fisheye highlighting, virtual word cloud and virtual small multiples for better comparisons
for massive targets instead of virtual glyphs~\cite{Chen2020}.

Fourth, there are a series of methods focus on the virtual targets in the virtual space or virtual screen,
which are more close to virtual reality (VR)~\cite{Sicat2019,Rhee2020,Lee2021} instead of AR.

%The most related work of \textit{ARLayout} are some authoring tools
%towards AR or VR visualizations~\cite{Sicat2019, Rhee2020, ElSayed2015, Lee2021a, Lee2021, Chen2020a, Chen2020}.
% As shown in Table~\ref{tab:discussion_relatedwork_cmp},
% we compare them from ten aspects:
% target number, virtual space, augmented information,
% searching, re-grouping (multi-attris), re-ranking (multi-attris),
% glyph vis, small multiples, fish eye highlight, single or collaborated.
% We summarize them into four criterias:
% data scale, task,
% visual presentation, workflow.


%~\cite{Sicat2019, Rhee2020, ElSayed2015, Lee2021a, Lee2021, Chen2020a, Chen2020}
%DXR：Sicat2019
%PapARVis：Chen2020a
%MarVisT：Chen2020
%AR Info Vis：Reipschlager2021
%VR Collab：Lee2021
%AVT：Rhee2020
%SA Vis：ElSayed2015


